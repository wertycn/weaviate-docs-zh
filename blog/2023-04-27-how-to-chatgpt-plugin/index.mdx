---
authors:
- shukri
- zain
date: 2023-04-27
description: A show-and-tell of how we created the Weaviate Retrieval Plugin for ChatGPT
image: ./img/hero.png
slug: how-to-chatgpt-plugin
tags:
- how-to
- integrations
title: How to Create Your Own ChatGPT Plugin
---

![如何创建ChatGPT插件](./img/hero.png)

<!-- 截断 -->

## 简介

在我们关于[ChatGPT插件](/blog/weaviate-retrieval-plugin)的博客中，我们谈到了插件的潜力，可以通过允许ChatGPT利用第三方资源来处理与其对话，从而扩展ChatGPT的功能。这些插件的价值在于它们帮助弥补ChatGPT目前存在的不足。例如，ChatGPT是建立在GPT 4.0之上的一个大型语言模型，它对数学和代数推理的理解不如对书面语言的理解，因此在需要解决数学问题时，使用WolframAlpha插件作为“数学模式”是完全合理的！

我们讨论过的ChatGPT的另一个缺点是，除非该上下文明确传达在提示的内容中，否则它在回答问题时不使用上下文。解决这个问题的方法是[ChatGPT检索插件](https://github.com/openai/chatgpt-retrieval-plugin)，它将ChatGPT连接到一个向量数据库，为上述问题提供了一个强大的解决方案。与ChatGPT连接的向量数据库可以用于存储和引用相关信息，在回答提示时充当LLM的长期记忆。

插件是一种非常强大的方式，您和我可以通过插件来改进LLM的用例，而无需重新训练基础的GPT模型。假设您正在使用ChatGPT，并意识到当您询问有关天气的问题时，它的对话能力不够好，或者它对您的健康情况没有足够专业的理解，无法根据您之前的血糖、血压水平和健康状况为您提供美味又健康的食谱建议。您可以创建一个插件来解决这些问题，并通过这样做来提高所有人的可用性，因为他们可以简单地安装您的插件并使用它！

那么唯一的问题是，**您如何获得OpenAI运行的独家插件alpha的访问权限**，以及**您如何创建ChatGPT的插件！** 不用担心，我们带来了两个好消息😀。

Weaviate正在与OpenAI和Cortical Ventures合作，在5月11日在波士顿的Hynes会议中心举办为期一天的[ODSC East生成式AI黑客马拉松活动](https://www.eventbrite.com/e/generative-ai-hackathon-odsc-east-2023-tickets-616336738777)。在那里，您将获得OpenAI提供的**OpenAI API**和**ChatGPT插件令牌**的访问权限，您还可以使用ChatGPT和Weaviate等工具创建自己的插件以及类似AutoGPT的应用程序，解决您关心的问题！您可以使用上面提供的链接进行注册，名额有限，请尽快注册！

![hackathon](./img/hackathon.png)

现在开始介绍如何为ChatGPT创建自己的插件。在这里，我们将逐步介绍创建Weaviate检索插件的过程。Weaviate检索插件将ChatGPT与Weaviate实例连接起来，允许其从向量数据库中查询相关文档，更新文档以便以后“记住”信息，以及删除文档以“忘记”它们！创建此插件的过程与创建通用插件的过程非常相似，因此我们认为这对于理解插件的创建过程非常有帮助，希望能对您有所帮助！

## 如何创建ChatGPT插件

![plugin](./img/plugin-light.png#gh-light-mode-only)
![plugin](./img/plugin-dark.png#gh-dark-mode-only)

完整的Weaviate检索插件的代码存储库位于[这里](https://github.com/weaviate/howto-weaviate-retrieval-plugin)。让我们一步一步地介绍，包括代码片段、我们遇到的一些挑战以及我们最终是如何解决它们的。

我们用于开发此插件的技术栈如下：
1. Python：使用Python编写所有内容
2. FastAPI：用于运行插件的服务器
3. Pytest：用于编写和运行测试
4. Docker：我们创建容器来构建、测试和部署插件

以下是我们开发插件所采取的步骤，第一部分专注于构建具有所需端点的Web应用程序，第二部分专门针对ChatGPT插件的开发，而第三部分则介绍了使用Fly.io进行远程部署的内容。我们按顺序介绍这些步骤，但根据您对材料的熟悉程度，可以随意跳过步骤。

## 第一部分：构建Web应用程序

**步骤1：设置开发环境**

为了设置我们的开发环境，我们使用了[Dev Containers](https://containers.dev/)。通过添加[Fly.io](https://fly.io/)、[Docker](https://www.docker.com/)和[Poetry](https://python-poetry.org/)，更新了`devcontainer.json`[文件](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.devcontainer/devcontainer.json)。您可以在[这里](https://containers.dev/templates)找到其他的开发容器模板。

**步骤2. 测试设置**

1. 在设置环境后，我们通过以下方式测试了一切是否正常：
创建一个[dummy endpoint](https://github.com/weaviate/howto-weaviate-retrieval-plugin/commit/57c00bc85aea3a155d330f0de72525ee26a665d1#diff-c16fbf0c6f7b90a46b94b36f88893c2d174476088608841f7254afba0e81373d)，当调用时它会简单地响应一个`{"Hello": "World"}`对象。

```python
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def read_root():
    """
    Say hello to the world
    """
    return {"Hello": "World"}
```

2. 使用PyTest设置测试，达到两个目标 - 首先，我们要检查我们的Weaviate实例是否正在运行，设置方法在[这里](https://github.com/weaviate/howto-weaviate-retrieval-plugin/commit/57c00bc85aea3a155d330f0de72525ee26a665d1#diff-e52e4ddd58b7ef887ab03c04116e676f6280b824ab7469d5d3080e5cba4f2128)，其次，我们要检查Fast API端点是否响应。这两个测试的定义在[这里](https://github.com/weaviate/howto-weaviate-retrieval-plugin/commit/57c00bc85aea3a155d330f0de72525ee26a665d1#diff-5f4f40de92f9f92f3638fdd1c3eace62db4d153ff64b915f82e43e6781960ed6)。

3. 我们还创建了一个[makefile](https://github.com/weaviate/howto-weaviate-retrieval-plugin/commit/57c00bc85aea3a155d330f0de72525ee26a665d1#diff-beda42571c095172ab63437d050612a571d0d9ddd3ad4f2aecbce907a9b7e3d0)来自动化运行和测试，并启动端点。在我们的makefile中，我们还指定了一个`run`命令，它将在本地启动服务器，以确保网络连接设置正确。您还可以连接到端口8000，这是FastAPI默认监听的端口，以检查连接性。

4. 最后一步是验证一切是否正常运行，您可以访问 `localhost:8000/docs`，这将为您提供用于您的端点的[Swagger UI](https://swagger.io/tools/swagger-ui/)。Swagger UI 允许您与服务器进行交互，与您定义的任何端点进行交互，并且所有内容都会实时更新 - 当我们以后想要手动调用端点与我们的 Weaviate 实例进行查询、更新和删除对象时，这将非常方便。

![swaggerui](./img/swagger.png)
完成以上步骤并确认一切都符合要求后，您可以开始实现特定插件的功能。

**第三步：实现一个函数来获取向量嵌入**

由于我们正在实现一个将向量数据库连接到ChatGPT的插件，我们需要定义一种生成向量嵌入的方法，当我们更新数据库中的文档时，可以生成和存储文档的向量嵌入。同时，当查询和执行向量搜索时，该函数也将用于将查询向量化。该函数的实现可在[此处](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/server/embedding.py)找到。
```python
import openai

def get_embedding(text):
    """
    Get the embedding for a given text
    """
    results = openai.Embedding.create(input=text, model="text-embedding-ada-002")

    return results["data"][0]["embedding"]
```

在这里，我们选择使用[`ada-002`模型](https://openai.com/blog/new-and-improved-embedding-model)，因为OpenAI指定了这个特定的模型用于他们的模板检索插件，但是由于查询是在向量数据库中进行的，我们也可以选择使用任何向量化器。

**步骤4：实现初始化Weaviate客户端和向量数据库的函数**

接下来，我们实现了一些函数来初始化 Weaviate Python 客户端，并通过客户端初始化 Weaviate 实例，通过检查模式是否存在，如果不存在，则添加一个模式。

请参考[这里](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/server/database.py)的代码实现。

```python
import weaviate
import os
import logging

INDEX_NAME = "Document"

SCHEMA = {
    "class": INDEX_NAME,
    "properties": [
        {"name": "text", "dataType": ["text"]},
        {"name": "document_id", "dataType": ["string"]},
    ],
}


def get_client():
    """
    Get a client to the Weaviate server
    """
    host = os.environ.get("WEAVIATE_HOST", "http://localhost:8080")
    return weaviate.Client(host)


def init_db():
    """
    Create the schema for the database if it doesn't exist yet
    """
    client = get_client()

    if not client.schema.contains(SCHEMA):
        logging.debug("Creating schema")
        client.schema.create_class(SCHEMA)
    else:
        class_name = SCHEMA["class"]
        logging.debug(f"Schema for {class_name} already exists")
        logging.debug("Skipping schema creation")
```

**步骤5：在服务器启动时初始化数据库并为Weaviate客户端添加依赖项**

现在我们需要集成这些函数的使用，以便在启动ChatGPT插件服务器时自动初始化Weaviate实例和客户端的连接。我们使用FastAPI的[lifespan](https://fastapi.tiangolo.com/advanced/events/#lifespan-events)功能在[主服务器Python脚本](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/server/main.py)中实现这一点，该脚本在每次服务器启动时运行。这个简单的函数调用了我们上面定义的数据库初始化函数，它返回Weaviate客户端对象。任何需要在服务器关闭时运行的逻辑可以在下面的`yield`语句之后添加。由于我们的插件不需要执行任何特定的操作，所以我们将其保留为空。

```python
from fastapi import FastAPI
from contextlib import asyncontextmanager

from .database import get_client, init_db

@asynccontextmanager
async def lifespan(app: FastAPI):
    init_db()
    yield


app = FastAPI(lifespan=lifespan)

def get_weaviate_client():
    """
    Get a client to the Weaviate server
    """
    yield get_client()
```

在此之后，初始的服务器设置和测试已经完成。现在我们来到了实现端点的有趣部分，这些端点将为ChatGPT提供与我们的插件进行不同交互方式的能力！

## 第二部分：实现OpenAI特定功能

![fun](./img/areyahavingfunmrkrabs.png)

**步骤1：开发Weaviate Retrieval插件特定的端点**

我们的插件有三个特定的端点：`/upsert`、`/query`和`/delete`。这些功能使ChatGPT能够将对象添加到Weaviate实例中，对Weaviate实例中的对象进行查询和搜索，以及在需要时删除对象。在启用插件的情况下与ChatGPT交互时，可以通过提示指示其使用特定的端点，但它也会独立决定何时使用适当的端点来完成对查询的响应！这些端点是扩展ChatGPT功能并使其能够与向量数据库进行交互的关键。

我们通过测试驱动开发的方式开发了这三个端点，因此我们将首先展示每个端点必须通过的测试，然后展示满足这些测试的实现。为了准备设置Weaviate实例以进行这些测试，我们通过一个测试数据集添加了以下测试文档：
```python
@pytest.fixture
def documents(weaviate_client):
    docs = [
        {"text": "The lion is the king of the jungle", "document_id": "1"},
        {"text": "The lion is a carnivore", "document_id": "2"},
        {"text": "The lion is a large animal", "document_id": "3"},
        {"text": "The capital of France is Paris", "document_id": "4"},
        {"text": "The capital of Germany is Berlin", "document_id": "5"},
    ]

    for doc in docs:
        client.post("/upsert", json=doc)
```

**实现`/upsert`端点：**

在使用`/upsert`端点之后，我们主要想要测试是否获得了适当的状态码，同时检查内容、id和向量是否都被正确地插入了。

以下是执行此操作的测试代码：

```python
def test_upsert(weaviate_client):
    response = client.post("/upsert", json={"text": "Hello World", "document_id": "1"})
    assert response.status_code == 200

    docs = weaviate_client.data_object.get(with_vector=True)["objects"]
    assert len(docs) == 1
    assert docs[0]["properties"]["text"] == "Hello World"
    assert docs[0]["properties"]["document_id"] == "1"
    assert docs[0]["vector"] is not None
```

下面的实现满足了所有这些要求和上面的测试：
```python
@app.post("/upsert")
def upsert(doc: Document, client=Depends(get_weaviate_client)):
    """
    Insert a document into weaviate
    """
    with client.batch as batch:
        batch.add_data_object(
            data_object=doc.dict(),
            class_name=INDEX_NAME,
            vector=get_embedding(doc.text),
        )

    return {"status": "ok"}
```

`/query` 和 `/delete` 端点的开发方式类似，如果您感兴趣可以阅读下面的内容！

<details>
  <summary>查看 /query 端点实现的详细信息。</summary>

**实现 `/query` 端点：**

对于这个端点，我们主要想检查它是否返回了正确数量的对象，并且我们期望的必需文档是否包含在返回的对象中。

```python
def test_query(documents):
    LIMIT = 3
    response = client.post("/query", json={"text": "lion", "limit": LIMIT})

    results = response.json()

    assert len(results) == LIMIT
    for result in results:
        assert "lion" in result["document"]["text"]
```

下面的实现将接受一个查询并返回一个检索到的文档和元数据列表。

```python
@app.post("/query", response_model=List[QueryResult])
def query(query: Query, client=Depends(get_weaviate_client)) -> List[Document]:
    """
    Query weaviate for documents
    """
    query_vector = get_embedding(query.text)

    results = (
        client.query.get(INDEX_NAME, ["document_id", "text"])
        .with_near_vector({"vector": query_vector})
        .with_limit(query.limit)
        .with_additional("certainty")
        .do()
    )

    docs = results["data"]["Get"][INDEX_NAME]

    return [
        QueryResult(
            document={"text": doc["text"], "document_id": doc["document_id"]},
            score=doc["_additional"]["certainty"],
        )
        for doc in docs
    ]
```
</details>

<details>
  <summary>查看/delete端点的实现细节。</summary>

**实现`/delete`端点：**

在这里，我们只是想检查返回的响应是否正确，并且在删除一个对象后，Weaviate实例中的总对象数量减少了一个。

```python
def test_delete(documents, weaviate_client):
    num_docs_before_delete = weaviate_client.data_object.get()["totalResults"]

    response = client.post("/delete", json={"document_id": "3"})
    assert response.status_code == 200

    num_docs_after_delete = weaviate_client.data_object.get()["totalResults"]

    assert num_docs_after_delete == num_docs_before_delete - 1
```

而端点的实现如下所示：

```python
@app.post("/delete")
def delete(delete_request: DeleteRequest, client=Depends(get_weaviate_client)):
    """
    Delete a document from weaviate
    """
    result = client.batch.delete_objects(
        class_name=INDEX_NAME,
        where={
            "operator": "Equal",
            "path": ["document_id"],
            "valueText": delete_request.document_id,
        },
    )

    if result["results"]["successful"] == 1:
        return {"status": "ok"}
    else:
        return {"status": "not found"}
```
</details>

在这里，我们向您展示了我们的端点是如何工作的。这将是您的插件最独特的地方，根据您想要实现的功能，您可以创建相应的端点并进行测试。

::::tip
Notice the docstrings we’ve included with all of our endpoints, these will be very important in the next step!
::::

**步骤 2: 准备插件清单文件**

这是您向OpenAI和ChatGPT指定插件公开的端点的位置，您可以在这里说明它们如何使用这些端点来完成特定任务，以及如果未正确使用这些端点时可能出现的错误等等！[OpenAI的指南](https://platform.openai.com/docs/plugins/getting-started/plugin-manifest)指出，您需要创建两个文件：[`openapi.yaml`文件](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.well-known/openapi.yaml)和[`ai-plugin.json`文件](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.well-known/ai-plugin.json)。

正如您所见，这两个文件都需要放在`.well-known`目录中，并且必须按照以下方式将其挂载到应用程序中，以便ChatGPT能够正确地使用它们。

`app.mount("/.well-known", StaticFiles(directory=".well-known"), name="static")`

让我们仔细看一下这两个文件：

`ai-plugin.json`

```json
{
    "schema_version": "v1",
    "name_for_human": "Weaviate Retrieval Plugin V2",
    "name_for_model": "Weaviate_Retrieval_Plugin",
    "description_for_human": "Plugin to interact with documents using natural language. You can query, add and remove documents.",
    "description_for_model": "Plugin to interact with documents using natural language. You can query, add and remove documents.",
    "auth": {
        "type": "user_http",
        "authorization_type": "bearer"
      },
    "api": {
        "type": "openapi",
        "url": "https://demo-retrieval-app.fly.dev/.well-known/openapi.yaml",
        "is_user_authenticated": false
    },
    "logo_url": "https://demo-retrieval-app.fly.dev/.well-known/logo.png",
    "contact_email": "support@example.com",
    "legal_info_url": "http://www.example.com/legal"
}
```
这个文件指定了应用程序的名称、标志资产等数据，还有更有趣的是，在`name_for_model`字段下，指定了模型（在本例中为ChatGPT/GPT4.0）将如何引用插件以及插件的描述`description_for_model`，这些描述可以被模型读取和理解。

`openapi.yaml`

![YAML](./img/openaiapi.png)

这个文件最重要的是指定了ChatGPT要使用的端点，并对每个端点进行了描述。

::::tip
Generating this `.yaml` file was quite challenging until we realized that you could simply generate the spec in json format by going to the SwaggerUI using `/docs` and clicking on the `/openapi.json` link. Then you can use this [website](https://www.json2yaml.com/) to convert from `.json` to `.yaml`.
::::


这两个文件对于ChatGPT正确理解和利用插件的暴露端点非常重要。

我们在实验中发现一个非常有趣的发现是，ChatGPT会读取这些文件，不仅了解何时使用端点，还了解如何正确使用它们！因此，如果ChatGPT没有正确使用您的端点，您应该尝试改进插件和端点的描述。OpenAI为创建这些描述提供了一些[最佳实践](https://platform.openai.com/docs/plugins/getting-started/writing-descriptions)。根据我们的实验，我们发现如果描述不足以描述如何使用端点，ChatGPT将以错误的语法调用端点，并在失败后重试。请参阅下面的示例：

![错误](./img/error.png)

**要点：** ChatGPT没有硬编码的指令来告诉你何时以及如何使用插件端点。您需要非常小心地描述您的插件和端点，以便ChatGPT可以按预期使用它们！FastAPI为您生成的`openapi.json`规范是基于您在代码中对端点进行文档化的方式，即函数的名称、文档字符串、查询描述和pydantic模型中字段的描述。完成这些步骤不在本博文的范围之内，请参考FastAPI文档以获取更多详细信息。通常情况下，您希望为您的插件编写完整和全面的文档，因为文档是确保正确使用插件的关键！

另外，当指定描述、文档字符串等内容时，您需要注意不要超过上下文长度，因为插件描述、API请求和API响应都会插入到与ChatGPT的对话中。这些都会占用模型的上下文限制。

**步骤3：在ChatGPT用户界面上进行插件的本地部署和测试**

允许http://localhost:8000和https://chat.openai.com向插件的服务器发起跨域请求。我们可以使用FastAPI的CORSMiddleware中间件轻松实现这一点。

```python
# for localhost deployment
if os.getenv("ENV", "dev") == "dev":
    origins = [
        f"http://localhost:8000",
        "https://chat.openai.com",
    ]

    app.add_middleware(
        CORSMiddleware,
        allow_origins=origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
```

上述代码仅在本地测试期间使用，并将使您在本地部署的应用程序能够与ChatGPT进行通信。请注意，我们的插件实际上有两个应用程序，根据所使用的身份验证类型而有所不同，对于本地部署和测试，我们不使用身份验证，而对于远程部署，我们使用Bearer令牌和https，稍后我们将进行说明。按照[此处](https://platform.openai.com/docs/plugins/getting-started/running-a-plugin)的详细说明，您可以在本地运行插件。这将允许您通过ChatGPT UI测试所有端点，并确保它们正常运行。

在继续下一步之前，先来看一下我们在本地进行的一些端点测试：

![test1](./img/test1.png)
![test2](./img/test2.png)
*确保upsert和query端点正常工作。请注意，根据我们在提示中使用的语言，ChatGPT将选择调用相应的端点。*

## 第三部分：远程部署到Fly.io

**步骤1：准备将插件远程部署到Fly.io**

一旦我们在本地满意地测试了插件，我们可以将其部署到远程并安装到ChatGPT中。以下是我们分享插件给那些有alpha访问权限的人所遵循的步骤：

1. 创建远程Weaviate实例：这是使用[Weaviate云服务](https://console.weaviate.cloud/)完成的。

2. 添加一个[dockerfile](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/Dockerfile)。这个dockerfile只是一个修改过的版本，基于[OpenAI提供的模板文件](https://github.com/openai/chatgpt-retrieval-plugin/blob/main/Dockerfile)，将用于远程设置您的环境并启动服务器。

3. 更新插件清单配置文件 `ai-plugin.json` 和 `openapi.yaml`，现在使用身份验证（[使用指南](https://platform.openai.com/docs/plugins/getting-started/plugin-manifest)）形式为Bearer令牌和您新创建的WCS实例，而不是localhost。

4. 更新应用程序，确保所有通信都经过身份验证。

您可以在[这里](https://github.com/weaviate/howto-weaviate-retrieval-plugin/commit/9ba22539a321e6cb8cb676c2e3a6f3a945a3f967)查看项目的完整差异，以了解它是如何准备好进行远程部署的。

**步骤2：在Fly.io上部署并在ChatGPT中安装**

这是最后一步，允许您将插件部署到Fly.io，在[这里](https://github.com/openai/chatgpt-retrieval-plugin/blob/main/docs/deployment/flyio.md)提供了详细的说明，可以按照文档中的指示进行操作。完成后，您可以在浏览器中打开ChatGPT，如果您有插件的Alpha测试权限，可以通过指定插件的托管URL并提供身份验证的Bearer令牌来安装插件。

## 结论

女士们先生们，这就是我们创建的用于增强ChatGPT的Weavaite检索插件的过程。创建不同的插件的过程非常相似，我们相信大部分步骤可以以类似的方式执行，唯一的变化可能在第二部分，即您定义特定于插件的端点的地方。

让我们通过可视化ChatGPT如何使用我们刚刚创建的插件来结束。下面的图示详细说明了如何使用`/query`端点。根据提示，它还可以调用`/delete`和`/upsert`端点。

![diagram](./img/diagram-light.png#gh-light-mode-only)
![diagram](./img/diagram-dark.png#gh-dark-mode-only)
*更具体地说，当用户提示ChatGPT时，它将查看`openapi.yaml`文件以读取终端描述，并决定在提示执行中使用的适当终端。在上面的例子中，它选择使用`/query`终端。然后，它将尝试构建正确的请求，并在失败时进行重试。查询将从Weaviate返回与原始提示相关的文档，ChatGPT将使用这些文档来回答原始提示！*

import WhatNext from '/_includes/what-next.mdx'

<WhatNext />