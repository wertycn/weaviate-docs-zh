---
authors:
- zain
date: 2022-12-27
description: Learn about the hardware, software and performance metric specifications
  behind our ~1B object import of the Sphere dataset into Weaviate.
image: ./img/hero.png
slug: details-behind-the-sphere-dataset-in-weaviate
tags:
- engineering
- concepts
title: The Details Behind the Sphere Dataset in Weaviate
---

![Weaviate中的Sphere数据集](./img/hero.png)

<!-- 截断 -->

在12月份的早些时候，我们写了一篇关于将Sphere数据集导入Weaviate的博文。在那篇文章中，我们介绍了Sphere数据集是什么，宣布了用于Weaviate的Sphere数据集文件的发布，并分享了如何使用Weaviate搜索大型数据集（如Sphere）的方法。具体来说，我们提供了一个简短的指南，介绍了如何使用Python和Spark将Sphere导入Weaviate，并提供了关于整个Sphere数据集的示例查询。如果您还没有查看过这篇文章，我们建议您快速浏览一下！

在本文中，我们将拉开帷幕，走进幕后，向您展示我们是如何将所有这些数据导入到Weaviate中的。您将看到从Sphere获取约10亿（8.98亿）篇文章摘要对象及其向量表示所需的所有细节。我们还将深入探讨所使用的硬件和软件设置，以及导入过程背后的性能指标。

如果您是：

* 对于将大型数据集或语料库导入Weaviate感兴趣
* 想知道如何使用Apache Spark（或pyspark）与Weaviate一起使用
* 想知道将如此大的数据集导入Weaviate所需的步骤

本文将至少回答其中一些问题。

我们注意到我们的目标是将10亿个对象导入Weaviate，以了解系统的行为。目前，对于大型数据集，RAM是最可能的瓶颈，因此我们使用了比实际需要更大的节点大小，以确保安全。

现在让我们来了解设置的详细信息！

## 设置

![Gear](./img/gear.png)

### 硬件：

为了设置我们的集群，我们配置了6个位于美国Central1区域（C区）的Google Kubernetes Engine（GKE）节点。每个节点都是m1-ultramem-40类型的实例，该实例是一个内存优化的实例，具有40个vCPU和961 GiB的内存。因此，我们的设置总共有240个vCPU和5766 GiB的内存。我们选择了内存优化实例，因为计算优化实例的内存不足。事后看来，我们可能可以使用更小的实例，因为每个实例的RAM使用峰值约为600GiB。

### 软件（Weaviate和Spark）：

#### Weaviate

我们在6个GKE节点上部署了6个Weaviate实例（每个节点一个实例，每个节点自动分片，总共6个分片），我们将机器上的所有资源都分配给了Weaviate。此外，每个Weaviate实例有1800 GiB的磁盘内存，并启用了`text2vec-huggingface`的矢量化模块。我们不需要对数据集进行任何矢量化操作，因为Sphere数据集已经预先计算了矢量化结果，矢量化模块仅用于矢量化查询。所使用的Weaviate版本是基于v1.16.0构建的，并且具有Hybrid Search功能的预览版。您可以在我们的[先前的文章](/blog/sphere-dataset-in-weaviate)中看到Hybrid Search的示例运行，并且在我们的[上周发布的v1.17版本](/blog/weaviate-1-17-release)中，我们正式发布了这一Hybrid Search功能，供您尝试！

#### 使用 Spark 连接器

为了将数据集导入Weaviate，我们使用了[Spark Connector](https://github.com/weaviate/weaviate-spark-connector)，而不是我们的客户端库之一。由于数据集的庞大大小，我们使用了Spark dataframe，并且Spark连接器可以有效地从Spark dataframe中填充Weaviate。另一个节省时间的功能是，连接器能够根据Spark DataType自动推断出Weaviate类模式的正确数据类型。（我们即将在我们的[YouTube频道](https://www.youtube.com/@Weaviate)上发布与Spark连接器的创建者的访谈，敬请关注！）

我们使用了一台64 GiB的Google Cloud Platform（GCP）n2实例来运行Spark。下面的步骤可以用来导入Sphere数据集中的数据。您可以参考这个[教程](/developers/weaviate/tutorials/spark-connector)了解导入代码的详细信息。

> 如果您按照本教程进行操作，请确保修改代码，只导入您敢导入的Sphere数据集的数据，因为这可能会成为一项非常昂贵的操作！
首先，使用`pyspark`库，您可以实例化一个连接，称为`SparkSession`:

```python
spark = (
    SparkSession.builder.config(
        "spark.jars",
        "gcs-connector-hadoop3-latest.jar,weaviate-spark-connector-assembly-v0.1.2.jar",
    )
    .master("local[*]")
    .appName("weaviate")
    .getOrCreate()
)
```

接下来，您需要将Sphere数据集导入到Spark dataframe中，通过加载我们提供的Parquet文件：


```
df = spark.read.parquet("gs://sphere-demo/parquet/sphere.899M.parquet")
```

接下来，启动一个Weaviate实例（或集群），连接到它并为Sphere数据集创建一个模式：

```python
client = weaviate.Client("http://localhost:8080")
client.schema.create_class(
    {
        "class": "Sphere",
        "properties": [
            {"name": "raw", "dataType": ["string"]},
            {"name": "sha", "dataType": ["string"]},
            {"name": "title", "dataType": ["string"]},
            {"name": "url", "dataType": ["string"]},
        ],
    }
)
```

然后将Spark dataframe的内容导入到Weaviate中：

```python
df.withColumnRenamed("id",
 "uuid").write.format("io.weaviate.spark.Weaviate") \
    .option("batchSize", 100) \
    .option("scheme", "http") \
    .option("host", "localhost:8080") \
    .option("id", "uuid") \
    .option("className", "Sphere") \
    .option("vector", "vector") \
    .mode("append").save()
```

当这个过程完成后，您的Weaviate实例应该已经完成了从Spark中摄取指定的数据，并且准备就绪。整个导入过程大约花费了48小时，但是您的时间可能会有所不同，这取决于您的设置和要导入的Sphere数据量。

通过上述设置，我们的主要目标是在导入10亿个对象时观察系统性能，但我们正在开发另一种可扩展的解决方案，使用诸如[Vamana](https://github.com/microsoft/DiskANN)和HNSW+PQ等技术。Vamana将允许基于磁盘的近似最近邻（ANN）搜索，而HNSW+PQ则使用HNSW构建和搜索一个图形，该图形覆盖了经过产品量化（PQ）编码和压缩的向量。这些发展都应该有助于在大规模内存使用方面的优化-如果您想深入了解，请阅读[这里](/blog/ann-algorithms-vamana-vs-hnsw)。随着新年的到来，我们将会有更多的信息，这只是我们的工程团队为您准备的一小部分内容！

## 导入性能指标
![放大镜](./img/magnifying-glass.png)

在本节中，让我们看一下与Sphere导入运行相关的一些统计数据。这将让您了解Weaviate集群在这个庞大的导入过程中的性能表现。

让我们从整个运行的俯视图开始。如上所述，整个过程大约需要48小时，在高层次上，系统的性能非常好。下面的图表显示了随时间导入的向量数量：

![导入的向量数量](./img/number-of-vectors-imported.png)

从斜率可以看出，即使导入的对象数量不同，导入速度的变化几乎是线性的，也就是说，我们几乎没有看到导入时间的明显延迟。需要注意的是，由于HNSW索引的时间复杂度大致是对数而不是线性的，所以这条线并不完全是直线。

为了对我们所处理的数据规模有所了解，让我们来看一下下面的柱状图。它显示了LSM存储的大小。

![LSM存储](./img/LSM-stores.png)

对象存储库存储对象及其向量，倒排索引支持BM25+过滤搜索。倒排索引不是严格必需的，但主要用于提供高效的过滤和存储空间所占用的价值。LSM存储的大小可达到数TB的数据量，然而Weaviate仍然表现出色！

现在，我们将放大并查看批量导入的15分钟窗口内的性能。为了提供一些背景，我们使用了22个并行批次，每个批次大小为100。平均批次持续时间约为1秒，非常适合工作，因为批次足够大，Weaviate能够接收足够的数据进行处理，而不会成为瓶颈，同时又足够小以最小化超时的风险。

![批量对象延迟](./img/batch-objects-latency.png)

如上所示，在上述时间窗口中，我们获得了出色的一致性性能，通常情况下批处理延迟较低，大部分情况下保持在一秒以下。在上周的v1.17版本发布中，通过启用内存表动态调整大小，使得大型数据集的导入速度更快，并且通过其他修复措施也消除了偶尔出现的延迟峰值。

这就是全部内容了！在这里，我们提供了硬件设置的详细信息，Spark连接器的使用以及在导入Sphere数据集期间的系统性能。希望您阅读这篇文章的过程中和我整理它的过程一样有趣！正如我之前提到的，如果这项工作让您感到兴奋，那么您会很高兴地知道我们在Weaviate厨房里还有更多相关工作的更新正在进行中🧑‍🍳。

import StayConnected from '/_includes/stay-connected.mdx'

<StayConnected />