---
authors:
- jerry
date: 2023-06-22
description: An introductory overview of LlamaIndex, the LLM framework
image: ./img/hero.png
slug: llamaindex-and-weaviate
tags:
- integrations
- how-to
title: LlamaIndex and Weaviate
---

![LlamaIndex和Weaviate](./img/hero.png)

<!-- truncate -->

虽然像GPT-4这样的大型语言模型在生成和推理方面具有令人印象深刻的能力，但它们在访问和检索特定事实、数据或上下文相关信息方面存在局限性。解决这个问题的一种常见方法是建立一个检索增强生成（RAG）系统：将语言模型与外部存储提供程序结合起来，创建一个整体软件系统，可以协调与这些组件之间的交互，以实现“与您的数据对话”的体验。

Weaviate和LlamaIndex的组合提供了构建强大可靠的RAG堆栈所需的关键组件，以便您可以轻松地在您的数据上提供强大的LLM功能体验，例如搜索引擎、聊天机器人等。首先，我们可以使用Weaviate作为外部存储提供者的向量数据库。接下来，在构建LLM应用程序时，我们可以使用强大的数据框架，如LlamaIndex，来协助Weaviate周围的数据管理和编排。

在这篇博客文章中，我们将概述LlamaIndex和一些核心数据管理和查询模块。然后，我们将通过一个初始的演示笔记本来进行介绍。

我们正在启动一个新系列的指南，指导您如何在LLM应用程序中使用LlamaIndex和Weaviate。

## LlamaIndex简介
LlamaIndex是用于构建LLM应用程序的数据框架。它提供了一个全面的工具包，用于导入、管理和查询外部数据，以便您可以在LLM应用程序中使用它。

### 数据导入
在数据摄取方面，LlamaIndex提供了100多个数据源的连接器，包括不同的文件格式（.pdf，.docx，.pptx），API（Notion，Slack，Discord等）以及网络爬虫（Beautiful Soup，Readability等）。这些数据连接器主要托管在[LlamaHub](https://llamahub.ai/)上。这使得用户可以轻松地集成他们现有的文件和应用程序中的数据。

### 数据索引化
一旦数据加载完成，LlamaIndex可以使用各种数据结构和存储集成选项（包括Weaviate）对这些数据进行索引。LlamaIndex支持对非结构化、半结构化和结构化数据进行索引。对非结构化数据进行索引的一种常见方法是将源文档拆分为文本“块”，将每个块进行嵌入，并将每个块/嵌入存储在向量数据库中。

### 数据查询
一旦您的数据被摄取/存储，LlamaIndex提供了定义高级检索/查询“引擎”的工具。我们的检索器构建允许您根据输入提示从知识库中检索数据。查询引擎构建允许您定义一个接口，可以接受输入提示，并输出一个增强了知识的响应 - 它可以在内部使用检索和合成 (LLM) 模块。

一些查询引擎“任务”的示例如下，按照从简单到高级的粗略顺序排列：
* 语义搜索：通过嵌入相似度到查询中，检索知识语料库中与查询最相似的前k个项目，并在这些上下文中综合生成响应。

* 结构化分析：将自然语言转换为可以执行的SQL查询。

* 查询分解为多个子问题：将查询分解为子问题，每个子问题针对底层文档的一个子集。每个子问题可以在自己的查询引擎中执行。

## 演示笔记本演练

让我们通过一个简单的示例来演示如何使用LlamaIndex和Weaviate构建一个简单的问答系统（QA）来处理Weaviate博客！

完整的代码可以在[Weaviate recipes repo](https://github.com/weaviate/recipes/tree/main/integrations/llamaindex/simple-query-engine)中找到。

第一步是设置您的Weaviate客户端。在这个示例中，我们通过端口`http://localhost:8080`连接到一个本地的Weaviate实例：

```python
import weaviate

# connect to your weaviate instance
client = weaviate.Client("http://localhost:8080")
```

下一步是将Weaviate文档导入并将文档解析成块。您可以选择使用我们的许多网页阅读器之一来自行抓取任何网站 - 但幸运的是，已经在recipes存储库中提供了已下载的文件。

```python
from llama_index.node_parser import SimpleNodeParser

# load the blogs in using the reader
blogs = SimpleDirectoryReader('./data').load_data()

# chunk up the blog posts into nodes
parser = SimpleNodeParser()
nodes = parser.get_nodes_from_documents(blogs)
```

在这里，我们使用SimpleDirectoryReader从给定的目录加载所有文档。然后，我们使用我们的`SimpleNodeParser`将源文档分割成Node对象（文本块）。

下一步是1）定义一个`WeaviateVectorStore`，并且2）使用LlamaIndex在该向量存储上构建一个向量索引。

```python
# construct vector store
vector_store = WeaviateVectorStore(weaviate_client = client, index_name="BlogPost", text_key="content")

# setting up the storage for the embeddings
storage_context = StorageContext.from_defaults(vector_store = vector_store)

# set up the index
index = VectorStoreIndex(nodes, storage_context = storage_context)
```

我们的WeaviateVectorStore抽象层在我们的数据抽象层和Weaviate服务之间创建了一个中央接口。请注意，`VectorStoreIndex`是从节点和包含Weaviate向量存储的存储上下文对象进行初始化的。在初始化阶段，节点被加载到向量存储中。

最后，我们可以在我们的索引之上定义一个查询引擎。该查询引擎将执行语义搜索和响应合成，并输出一个答案。

```python

​​query_engine = index.as_query_engine()
response = query_engine.query("What is the intersection between LLMs and search?")
print(response)
```

您应该会得到以下类似的答案：

```
The intersection between LLMs and search is the ability to use LLMs to improve search capabilities, such as retrieval-augmented generation, query understanding, index construction, LLMs in re-ranking, and search result compression. LLMs can also be used to manage document updates, rank search results, and compress search results. LLMs can be used to prompt the language model to extract or formulate a question based on the prompt and then send that question to the search engine, or to prompt the model with a description of the search engine tool and how to use it with a special `[SEARCH]` token. LLMs can also be used to prompt the language model to rank search results according to their relevance with the query, and to classify the most likely answer span given a question and text passage as input.
```

## 接下来的系列内容

本博客文章分享了关于LlamaIndex和Weaviate集成的初步概述。我们介绍了LlamaIndex提供的工具包，并提供了一个关于如何在Weaviate的博客文章上构建一个简单的问答引擎的笔记本。现在我们已经有了基础的理解，接下来我们将分享更高级的指南。请保持关注！

import WhatNext from '/_includes/what-next.mdx'

<WhatNext />