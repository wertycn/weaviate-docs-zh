---
authors:
- connor
- erika
date: 2023-06-13
description: Learn about the intersection between LLMs and Search
image: ./img/hero.png
slug: llms-and-search
tags:
- search
- concepts
title: Large Language Models and Search
---

![LLMs and Search](./img/hero.png)

<!-- truncate -->

最近在大型语言模型（LLM）技术方面取得的突破，正在为软件的许多领域带来转变。搜索和数据库技术尤其与LLMs有着有趣的纠缠关系。有些情况下，搜索可以提升LLMs的能力，反之亦然，LLMs也可以提升搜索的能力。在本博文中，我们将分解LLMs和搜索之间的5个关键组成部分。

* 增强检索生成
* 查询理解
* 索引构建
* 语言模型在重新排序中的应用
* 搜索结果压缩

*最后，我们将对生成式反馈循环在这个过程中的应用进行总结。*

## 增强检索生成
自从大型语言模型（LLM）问世以来，开发人员和研究人员一直在测试将信息检索和文本生成能力结合起来的可能性。通过将搜索引擎与LLM相结合，我们不再需要对LLM进行精细调整以处理特定的数据。这种方法被称为[检索增强生成](https://arxiv.org/abs/2005.11401)（RAG）。


没有RAG，LLM只能根据其训练数据来生成文本，而无法在训练截止后引入新的信息。 [Sam Altman](https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshape-society-acknowledges/story?id=97897122) 表示“我们创建的模型应该被看作是一个推理引擎，而不是一个事实数据库。”基本上，我们应该只使用语言模型的推理能力，而不是它所具有的知识。

RAG 相对传统语言建模具有以下 4 个主要优势：

1. 它使语言模型能够在没有梯度下降优化的情况下推理新数据。
2. 更新信息更加容易。
3. 更容易追溯生成结果的来源（也称为“引用其来源”），并解决幻象问题。
4. 它**减少**了推理所需的参数数量。参数数量是导致这些模型在某些用例中昂贵且不实际的原因（在[Atlas](https://arxiv.org/abs/2208.03299)中有展示）。

### 语言模型如何发送搜索查询

让我们深入了解一下如何将提示转化为搜索查询。传统的方法是将提示本身直接作为查询。通过语义向量搜索的创新，我们可以向搜索引擎发送更加简洁的查询。例如，我们可以通过搜索“法国的地标”来找到关于“埃菲尔铁塔”的信息。

然而，有时候我们可以通过提出问题来得到更好的搜索结果，而不是直接发送原始提示。这通常是通过提示语言模型根据提示提取或形成一个问题，然后将这个问题发送给搜索引擎来实现的，比如将“法国的地标”翻译成“法国有哪些地标？”或者将用户的提示“请给我写一首关于法国地标的诗”翻译成“法国有哪些地标？”然后发送给搜索引擎。

我们还可以要求语言模型根据需要自行生成搜索查询。解决方案之一是使用一个特殊的 `[SEARCH]` 标记，向模型提示搜索引擎工具的描述以及如何使用它。然后我们将语言模型的输出解析为搜索查询。在复杂问题需要分解为子问题的情况下，使用 `[SEARCH]` 标记触发模型也非常方便。LlamaIndex 在他们的框架中实现了这个功能，称为 [sub question query engine](https://gpt-index.readthedocs.io/en/latest/reference/query/query_engines/sub_question_query_engine.html)。它将复杂查询分解为多个子问题，并利用多个数据源收集响应来形成最终答案。

另一种有趣的技术是根据语言模型输出的确定性触发额外的查询。简单来说，语言模型是一种概率模型，它为文本序列中最可能的下一个词分配概率。FLARE技术会对可能的下一个句子进行采样，将每个词的概率相乘，如果结果低于一定阈值，则触发带有先前上下文的搜索查询。有关[FLARE技术的更多信息，请点击此处](https://www.youtube.com/watch?v=IVYwLLDABzI)。

有些查询需要非常彻底的搜索或需要进行精细化处理。"思维链"是一种常用的方法，用于协助语言模型的推理能力。[AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT)展示了模型如何在很少人为干预的情况下自主运行以完成各种任务。这是通过使用思维链推理将任务分解为中间步骤来实现的。程序会持续运行，直到完成所有任务。

在这种思路中的另一个前沿理念是[Tree-of-Thoughts抽样](https://arxiv.org/abs/2305.10601)。在Tree-of-Thoughts搜索中，每个节点代表语言模型的一个提示。然后，我们通过抽样多个随机生成来分支树。该算法与一个决策节点耦合，决定要剪掉哪些分支，或者何时发现了一个令人满意的答案。下面是来自Yao等人的Tree-of-Thoughts的示意图:

![思维树](./img/tree-of-thought.png)

## 查询理解

与盲目地向搜索引擎发送查询不同，矢量数据库（如Weaviate）提供了许多可调整的参数以获得更好的搜索结果。首先，一个查询可能更适合进行符号聚合，而不是语义搜索。例如：`“美国乡村歌手的平均年龄是多少？”` 这个查询更适合使用SQL查询来回答：`SELECT avg(age) FROM singers WHERE genre=”country” AND origin=”United States of America”`。

为了弥合这个差距，LLMs可以被提示将这些类型的问题翻译成SQL语句，然后可以针对数据库执行。类似地，Weaviate有一个[Aggregate API](/developers/weaviate/api/graphql/aggregate)，可以用于处理与非结构化数据关联的符号化元数据的这类查询。

除了Text-to-SQL主题外，还有一些与向量搜索相关的参数需要调整。首先是我们想要搜索哪个类别？在Weaviate中，我们可以将数据分成不同的类别，每个类别都有一个独立的向量索引，以及一组独特的符号属性。接下来，在向量搜索时我们需要决定添加哪些过滤器。一个很好的例子是基于价格对搜索结果进行过滤。我们不仅想要与查询最相似的物品，还想要那些价格低于100美元的物品。LlamaIndex的[查询引擎](https://gpt-index.readthedocs.io/en/latest/use_cases/queries.html#multi-document-queries)提供了非常好的抽象，可以利用这个LLM查询理解的概念。

在下面的示例中，LLM添加了筛选器，其中“animal”=“dog”，以便于搜索有关Goldendoodles的信息。这是通过向LLM提供关于数据模式和在Weaviate中格式化结构化向量搜索的语法的信息来完成的。

<img
    src={require('./img/self-query.gif').default}
    alt="alt"
    style={{ width: "100%" }}
/>

## 索引构建
大型语言模型还可以彻底改变我们为搜索引擎索引数据的方式，从而提高搜索质量。在这里有4个关键领域需要考虑：
1. 摘要索引
2. 提取结构化数据
3. 文本分块
4. 管理文档更新。

### 摘要索引
向量嵌入模型通常一次性只能处理512个标记。这限制了向量嵌入在表示长文档（如整章的书或整本书）方面的应用。一种新兴的解决方案是应用摘要链，例如在本文末尾展示的创建和优化示例，来对长文档进行摘要。然后，我们对摘要进行向量化，并建立索引，以便将语义搜索应用于比较长文档（如书籍或播客转录）。

### 提取结构化数据
LLM在搜索索引构建中的下一个应用是从非结构化文本块中提取结构化数据。假设我们从迈阿密热火队球员Jimmy Butler的维基百科页面中提取了以下文本块：

![维基百科示例](./img/wiki-example.png)

现在我们按照以下方式提示大型语言模型：

```
Given the following text chunk: `{text_chunk}`
Please identify any potential symbolic data contained in the passage. Please format this data as a json dictionary with the key equal to the name of the variable and the value equal to the value this variable holds as evidenced by the text chunk. The values for these variables should either have `int`, `boolean`, or `categorical` values.
```

对于这个例子，使用`text-davinci-003`和温度为0，会产生以下输出:

```json
{ "Draft Pick": 30, "Most Improved Player": true, "Teams": ["Chicago Bulls", "Minnesota Timberwolves", "Philadelphia 76ers", "Miami Heat"], "NBA Finals": 2, "Lead League in Steals": true }
```

我们可以将这个提示应用到维基百科文章中的每个文本块上，然后聚合所发现的符号属性。然后，我们可以回答关于我们的数据的符号性问题，比如，“有多少NBA球员曾效力于超过3个不同的NBA球队？”我们还可以像上面描述的那样，使用这些过滤器进行向量搜索，与Llama Index的“查询引擎”相关。

### 符号数据到文本
在LLM出现之前，世界上的大部分数据都是以表格数据集的形式组织的。表格数据集描述了`数值`、`布尔`和`分类`变量的组合。例如，“年龄”=26(`数值`)，“吸烟者”=True(`布尔`)，或者“喜爱的运动”=“篮球”(`分类`)。一种令人兴奋的新技术是通过提示一个大型语言模型将表格数据转化为文本。然后，我们可以使用来自OpenAI、Cohere、HuggingFace和其他平台的现成模型对这个文本描述进行向量化，以实现语义搜索。最近，我们在[AirBnB列表](https://weaviate.io/blog/generative-feedback-loops-with-llms)中提出了这个想法的一个例子，将关于每个房产价格、街区等的表格数据转化为文本描述。非常感谢Svitlana Smolianova为我们创建了以下概念动画。

<img
    src={require('./img/gen-example.gif').default}
    alt="alt"
    style={{ width: "100%" }}
/>

### 文本分块
类似于将文本块向量化的512个标记长度，我们可以考虑使用大型语言模型来确定切割文本块的好位置。例如，如果我们有一个项目列表，将列表分成两个块可能不是最佳实践，因为第一半会落在一个块[:512]的末尾循环中。这也类似于自动识别长序列文本（如播客、YouTube视频或文章）中的关键主题的提示。
### 管理文档更新
LLM在这个领域中解决的另一个新兴问题是搜索索引的持续维护。最好的例子可能是代码仓库。往往一个拉取请求可能会完全改变一个已经被矢量化并添加到搜索索引中的代码块。我们可以通过询问LLM来触发操作：这个改变足以触发重新矢量化吗？我们应该更新与此块相关联的符号属性吗？因为这个更新，我们应该重新考虑上下文的分块吗？此外，如果我们将更大上下文的摘要添加到这个块的表示中，这个更新足够重要吗，需要重新触发对整个文件进行摘要？

## 重新排序中的语言模型（LLMs）
搜索通常在管道中进行操作，这是一个描述有向无环图（DAG）计算的常见术语。如前所述，`查询理解`是这个管道中的第一步，通常紧随其后的是检索和重新排序。

正如我们在[之前的文章](https://weaviate.io/blog/ranking-models-for-better-search)中所描述的，重新排序模型是零-shot泛化场景中的新兴技术。重新排序器的故事大多涉及将表格形式的用户特征与表格形式的产品或物品特征结合起来，然后输入XGBoost模型进行训练。这需要大量的用户数据才能实现，而零-shot泛化可能会对此产生影响。

交叉编码器通过将`(查询, 文档)`对作为输入，并输出高精度的相关性得分而变得流行。这也可以很容易地推广到推荐领域，其中排序器以`(用户描述, 物品描述)`对作为输入。

LLM Re-Rankers有望通过接收多个文档来改变Cross Encoders，以进行高精度的相关性计算。这使得transformer LLM能够在潜在结果列表上应用注意力。这也与LLM的AutoCut概念密切相关，它指的是给语言模型k个搜索结果，并提示其确定其中有多少个搜索结果足够相关，可以显示给人类用户，或者传递到LLM计算链的下一个步骤。

回到XGBoost符号重新排序的话题，LLM重新排序器在符号重新排序方面也有很好的定位，能够实现创新。例如，我们可以这样激活语言模型：

```
Please rank the following search results according to their relevance with the query: {query}.

Please boost relevance based on recency and if the Author is “Connor Shorten”.
```

每个搜索结果都以键值数组的形式打包，其中包含与其关联的元数据。这样做的额外好处是，业务实践者可以轻松更换排名逻辑。这也大大增加了推荐系统的可解释性，因为LLMs可以很容易地被提示提供排名的解释，而不仅仅是排名本身。

## 搜索结果压缩

传统上，搜索结果以长列表的形式呈现给人类用户，包括相关的网站或段落。这就需要额外的努力来筛选结果，以找到所需的信息。新技术如何减少这种额外的努力呢？我们将通过问答和摘要的视角来探讨这个问题。

### 问答

即使在最近LLM的突破之前，提取式问答模型已经开始在这方面进行创新。提取式问答描述了根据问题和文本段落作为输入，对最可能的答案范围进行分类的任务。类似于ChatGPT无需在数据上进行昂贵的训练而能够推理出有关数据的能力，许多现成的提取式问答模型在未在训练过程中见过的新的（问题，文本段落）对上都能够很好地泛化。下面是一个示例：

![问答示例](./img/qa-example.png)
[来源](https://rajpurkar.github.io/SQuAD-explorer/)

抽取式问答的一个好处是没有虚构信息的可能性。虚构信息是指机器学习模型在回答中编造信息的情况，在这种技术中是不可能发生的，因为它完全受限于输入的上下文。这并不意味着它的答案不可能是错误的。
然而，与此相反的是，这些模型受到上下文的限制，因此无法对复杂的问题进行推理，并组合通过段落组装的答案的部分。因此，除非我们有一个像“诺曼人这个词的原始含义是什么？”这样直截了当的问题，LLM提供了显著的新能力。

另一个关键组成部分是LLM能够跨多个段落组合信息的能力。例如，我们可以应用Map Reduce提示，如：

```
Please summarize any information in the search result that may help answer the question: {query}

Search result: {search_result}
```

然后使用以下方法将每个段落的中间响应汇总起来:

```
Please answer the following question: {query} based on the following information {information}
```

也许我们可以想象将“map reduce”中的“reduce”部分应用于从每个段落中提取的信息。这可能是一种更便宜的方法来处理“map”步骤，但它不太可能效果像样，并且不提供与改变“map”提示从每个段落中提取的内容的灵活性。

<img
    src={require('./img/map-reduce.gif').default}
    alt="alt"
    style={{ width: "100%" }}
/>

### 摘要
与使用LLM进行问答类似，当我们考虑多个长度超过512个标记的段落与单个段落进行比较时，使用LLM进行摘要提供了AI系统的一个重要新功能。

```
Please summarize the search results for the given query: {query}

You will receive each search result one at a time, as well as the summary you have generated so far.

Current summary: {current_summary}

Next Search Result: {search_result}
```

<img
    src={require('./img/refine.gif').default}
    alt="alt"
    style={{ width: "100%" }}
/>

## 关键思想回顾
以下是我们在大型语言模型和搜索交叉领域提出的关键概念的快速回顾：

* **检索增强生成：** 通过从搜索引擎检索额外信息来增强LLM的输入。关于如何触发这个检索器，例如自问提示或FLARE解码，有越来越多的细微差别。
* **查询理解：** LLM（语言模型）可以可靠地将自然语言问题转化为SQL语句进行符号聚合。这种技术也可以用于通过选择要搜索的“类别”以及要添加到搜索的“筛选器”来导航Weaviate。
* **索引构建：** LLMs可以将信息转化为便于构建搜索索引的形式。这可以包括对长内容进行概述、提取结构化数据、识别文本块、管理文档更新，或将结构化数据转化为文本以进行向量化处理。
* **LLMs在重排中的应用:** 排名模型与检索模型不同，明确将查询和/或用户描述以及每个候选文档作为输入，通过神经网络输出一个细粒度的相关性分数。LLMs可以在没有额外训练的情况下完成这项任务。有趣的是，我们还可以通过指定符号偏好，如“价格”或“最新”，提示LLMs进行排名，而不仅仅是基于非结构化的文本内容。
* **搜索结果压缩：** LLM（语言模型）为人类用户提供了新的搜索体验。与浏览搜索结果不同，LLMs可以应用MapReduce或摘要链来解析数百个搜索结果，并向人类用户或其他LLMs展示更密集的最终结果。

## 生成性反馈循环
在我们[关于生成反馈循环的第一篇博客文章](https://weaviate.io/blog/generative-feedback-loops-with-llms)中，我们分享了一个示例，展示了如何为AirBnB的房源生成个性化广告。它会根据用户的偏好找到相关的房源，并生成相应的广告，并通过交叉引用将其链接回数据库。这很有趣，因为我们使用语言模型的输出，并对结果进行索引以供以后检索。

生成式反馈循环（Generative Feedback Loops）是我们正在开发的一个术语，用于广泛指称将LLM推理的输出保存回数据库供将来使用的情况。本博客文章涉及了几种这样的情况，例如撰写播客摘要并将结果保存回数据库，以及从非结构化数据中提取结构化数据并保存回数据库。

<img
    src={require('./img/gfl.gif').default}
    alt="alt"
    style={{ width: "100%" }}
/>

## 关于LLM框架的评论
LLM框架，例如[LlamaIndex](https://gpt-index.readthedocs.io/en/latest/)和[LangChain](https://python.langchain.com/en/latest/)，通过在项目中使用共同的抽象来控制新兴LLM应用的混乱。我们在文章的结尾要对这些框架的构建者表示衷心的感谢，他们极大地帮助了我们对这个领域的理解。还要特别感谢Greg Kamradt和Colin Harmon在[Weaviate Podcast #51](https://www.youtube.com/watch?v=iB4ki6gdAdc)上对这些主题的讨论。


import WhatNext from '/_includes/what-next.mdx'

<WhatNext />