---
authors:
- dan
date: 2023-01-16
description: Get an intuitive understanding of what exactly vector embeddings are,
  how they're generated, and how they're used in semantic search.
image: ./img/hero.png
slug: vector-embeddings-explained
tags:
- concepts
title: Vector Embeddings Explained
---

Weaviate的核心功能是提供高质量的搜索结果，不仅仅局限于简单的关键字或同义词搜索，而是实际上找到用户查询的含义，或者对用户提出的问题提供实际答案。

<!-- truncate -->

语义搜索（以及问答）本质上是通过相似性进行搜索，例如通过文本的含义或图像中包含的对象进行搜索。例如，考虑一个包含葡萄酒名称和描述的图书馆，其中一个描述提到这款葡萄酒“搭配**鱼**很好”。一个“适合**海鲜**”的关键词搜索，甚至是一个近义词搜索，都无法找到这款葡萄酒。一个基于含义的搜索应该理解到“鱼”和“海鲜”是相似的，而“good with X”表示葡萄酒适合“X” —— 于是应该能找到这款葡萄酒。

![向量嵌入示例](./img/vector-embeddings-example.png)

计算机如何模拟我们对语言的理解以及单词或段落之间的相似性？为了解决这个问题，语义搜索在其核心使用一种叫做**向量嵌入**（或简称为**向量**或**嵌入**）的数据结构，它是一个由数字组成的数组。下面是上述语义搜索的逐步工作过程：

1. [向量数据库](/blog/vector-library-vs-vector-database)在将每个数据对象插入或更新到数据库时，使用给定的模型计算一个向量嵌入。
2. 将嵌入放入索引中，以便数据库可以[快速](/blog/why-is-vector-search-so-fast)执行搜索。
3. 对于每个查询，
    1. 使用与数据对象相同的模型计算一个向量嵌入。
    2. 使用特殊算法，数据库找到与查询计算得出的给定向量最接近的向量。[点击此处](/blog/distance-metrics-in-vector-search)了解更多关于向量搜索中的距离度量的信息。

搜索的质量主要取决于模型的质量 - 这是"秘密酱料"，因为许多模型仍然是[闭源的](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)。搜索的速度主要取决于Weaviate，这是一个开源项目，[不断改进其性能](/blog/weaviate-1-18-release)。

## 什么是向量嵌入？
向量是数据的数值表示，可以捕捉数据的某些特征。例如，在文本数据的情况下，“cat”和“kitty”具有相似的含义，即使将单词“cat”和“kitty”逐字进行比较，它们在字母上是非常不同的。为了使语义搜索有效工作，必须能够充分捕捉“cat”和“kitty”之间的语义相似性。这就是使用向量表示以及它们的推导如此重要的原因。

在实践中，向量是由机器学习模型生成的具有固定长度的实数数组（通常为数百到数千个元素）。为数据对象生成向量的过程称为向量化。Weaviate使用[模块](/developers/weaviate/modules/retriever-vectorizer-modules)（如OpenAI、Cohere、Google PaLM等）生成向量嵌入，并将对象和向量方便地存储在同一个数据库中。例如，将上述两个单词向量化可能会得到：

```text
cat = [1.5, -0.4, 7.2, 19.6, 3.1, ..., 20.2]
kitty = [1.5, -0.4, 7.2, 19.5, 3.2, ..., 20.8]
```

这两个向量具有非常高的相似性。相比之下，“banjo”或“comedy”的向量与这两个向量都不太相似。在这种程度上，向量捕捉了单词的语义相似性。

既然您已经了解了向量是什么，以及它们可以在一定程度上表示含义，您可能会有进一步的问题。首先，每个数字代表什么意思？这取决于生成向量的机器学习模型，并不一定清晰，至少从我们人类对语言和含义的理解来看。但是，我们有时可以通过将向量与我们熟悉的单词相关联来获得一个大致的概念。

基于向量的意义表示在几年前引起了相当大的轰动，通过揭示单词之间的数学操作。也许最著名的结果是

    “king − man + woman ≈ queen”

这表明“king”和“man”之间的差异是一种“王权”，这种差异类似地应用于“queen”减去“woman”。 Jay Alamar提供了一个有用的[可视化](https://jalammar.github.io/illustrated-word2vec/)来说明这个方程。几个概念（“woman”，“girl”，“boy”等）被向量化为（由）使用[GloVe模型](https://en.wikipedia.org/wiki/GloVe)生成的50个数字的数组。在[向量术语](https://en.wikipedia.org/wiki/Vector_(mathematics))中，这50个数字被称为维度。这些向量使用颜色进行可视化，并排在每个单词旁边：

![向量嵌入可视化](./img/vector-embeddings-visualization.png)
*来源：Jay Alamar*

我们可以看到所有单词在一个维度上共享一个深蓝色的列（虽然我们无法确定它代表什么），而单词“water”在外观上与其他单词有很大的不同，这是合理的，因为其他单词都是人名。此外，“girl”和“boy”在外观上更相似，而不是和“king”和“queen”分别相似，而“king”和“queen”在外观上也相似。

因此，我们可以*看到*这些词向量嵌入与我们对含义的直观理解相吻合。更令人惊奇的是，向量嵌入不仅仅局限于表示单词的含义。

实际上，可以从任何类型的数据对象生成有效的向量嵌入。文本是最常见的，其次是图像，然后是音频（这是Shazam根据短而甚至嘈杂的音频片段识别歌曲的方式），还有时间序列数据、3D模型、视频、分子等。嵌入是这样生成的，使得具有相似语义的两个对象在向量空间中的向量是“接近”的，即它们之间的距离“小”（请参考[向量空间模型](https://en.wikipedia.org/wiki/Vector_space_model)）。这个距离可以用多种方式计算（请参考[向量搜索中的距离度量](/blog/distance-metrics-in-vector-search)），其中最简单的一种是“在每个向量的第`i`个位置上元素之间的绝对差值之和”（请记住，所有向量具有相同的固定长度）。

让我们来看一些数字（不涉及数学，保证！），并用另一个文本示例进行说明：

对象（数据）：包括 `cat`、`dog`、`apple`、`strawberry`、`building`、`car` 的单词

搜索查询：`fruit`

一组简单的向量嵌入（仅有 5 个维度）用于对象和查询可能如下所示：

| 单词               | 向量嵌入                        |
|--------------------|---------------------------------|
| `cat`              | `[1.5, -0.4, 7.2, 19.6, 20.2]`  |
| `dog`              | `[1.7, -0.3, 6.9, 19.1, 21.1]`  |
| `apple`            | `[-5.2, 3.1, 0.2, 8.1, 3.5]`    |
| `strawberry`       | `[-4.9, 3.6, 0.9, 7.8, 3.6]`    |
| `building`         | `[60.1, -60.3, 10, -12.3, 9.2]` |
| `car`              | `[81.6, -72.1, 16, -20.2, 102]` |
| **查询: `水果`**   | `[-5.1, 2.9, 0.8, 7.9, 3.1]`    |

如果我们观察向量的每个元素，我们可以很快地发现`cat`和`dog`比`dog`和`apple`更接近（我们甚至不需要计算距离）。同样地，`fruit`比其他词更接近`apple`和`strawberry`，因此它们将是“fruit”查询的前几个结果。

但这些数字是从哪里来的呢？这就是真正的魔力所在，也是现代深度学习的进展所产生巨大影响的地方。

## 向量嵌入是如何生成的？

向量搜索的神奇之处主要在于如何为每个实体和查询生成嵌入向量，其次是如何在非常大的数据集中高效搜索（有关后者，请参阅我们的[“为什么向量搜索如此快速”](/blog/why-is-vector-search-so-fast)文章）。

正如我们之前提到的，向量嵌入可以针对文本、图像、音频等各种媒体类型生成。对于文本来说，向量化技术在过去十年中发展迅速，从备受推崇的[word2vec](https://en.wikipedia.org/wiki/Word2vec)（[2013年](https://code.google.com/archive/p/word2vec/)）到最新的Transformer模型时代，其中最重要的里程碑是[BERT](https://en.wikipedia.org/wiki/BERT_(language_model))的发布（[2018年](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)）。

### 单词级别的稠密向量模型（word2vec、GloVe等）
[word2vec](https://wiki.pathmind.com/word2vec) 是一种模型架构系列，它引入了语言处理中“稠密”向量的概念，其中所有值都是非零的。

Word2vec特别使用神经网络模型从大量文本语料库中学习词语关联（最初由Google使用1000亿个单词进行训练）。它首先从语料库中创建词汇表，然后使用通常为300维的向量表示学习词语。在向量空间中，上下文中出现相似的词语具有接近的向量表示，但是词汇表中的每个词语只有一个对应的词向量。因此，词语的含义可以量化 - “run”和“ran”被认为比“run”和“coffee”更相似，但是像“run”这样具有多个含义的词语只有一个向量表示。

正如其名称所示，word2vec是一个词级别的模型，无法单独产生用于表示更长的文本（如句子、段落或文档）的向量。然而，可以通过聚合组成词的向量来实现这一点，通常会通过引入权重来将某些词比其他词更加重视。

然而，word2vec仍然存在重要的限制：
* 它不处理具有多个含义的单词（多义词）：“run”、“set”、“go”或“take”每个都有[300多个含义](https://www.insider.com/words-with-the-most-definitions-2019-1)！
* 它不处理含糊不清含义的单词：“咨询”可以是其自身的反义词，就像[许多其他单词](http://www.dailywritingtips.com/75-contronyms-words-with-contradictory-meanings/)一样。

这将带我们进入下一个最先进的模型。

### Transformer模型（BERT、ELMo和其他模型）
当前最先进的模型基于所谓的“transformer”架构，该架构是在[这篇论文](https://arxiv.org/abs/1706.03762)中介绍的。

[Transformer模型](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))，如BERT及其后续模型，通过考虑每个单词的上下文来改善搜索准确性、[精确率和召回率](https://en.wikipedia.org/wiki/Precision_and_recall)来创建完整的上下文嵌入（尽管[BERT成功的确切机制尚未完全理解](https://aclanthology.org/D19-1445/)）。与上下文无关的word2vec嵌入不同，transformer生成的嵌入考虑了整个输入文本的内容-每个单词的每次出现都有自己的嵌入，受到周围文本的影响而被修改。这些嵌入更好地反映了词语的多义性，只有在考虑上下文时才能消除歧义。

一些潜在的缺点包括：
* 增加的计算要求：微调Transformer模型速度较慢（需要几小时而不是几分钟）
* 增加的内存要求：上下文敏感性大大增加了内存需求，这通常导致输入长度的限制。

尽管存在这些缺点，Transformer模型仍然取得了巨大的成功。近年来，无数的文本矢量化模型不断涌现出来。此外，还有许多其他数据类型（如音频、视频和图像）的矢量化模型存在。一些模型，比如[CLIP](https://openai.com/blog/clip/)，能够将多种数据类型（在这种情况下是图像和文本）矢量化到一个向量空间中，从而可以只使用文本来通过其内容搜索图像。

## 使用Weaviate进行向量嵌入

因此，Weaviate被配置为支持多种不同的向量化模型和向量化服务提供商。您甚至可以[使用自己的向量](/developers/weaviate/tutorials/custom-vectors)，例如，如果您已经有一个可用的向量化流程，或者如果公开可用的模型都不适合您。

首先，Weaviate通过我们的`text2vec-hugginface`模块支持使用任何Hugging Face模型，因此您可以选择[Hugging Face上发布的众多句子转换器之一](/blog/how-to-choose-a-sentence-transformer-from-hugging-face)。或者，您可以通过[`text2vec-openai`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai)或[`text2vec-cohere`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere)模块使用其他非常流行的矢量化API，如OpenAI或Cohere。您甚至可以使用[`text2vec-transformers`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers)在本地运行转换器模型，而[`multi2vec-clip`](/developers/weaviate/modules/retriever-vectorizer-modules/multi2vec-clip)等模块可以使用CLIP模型将图像和文本转换为向量。

但是它们都执行相同的核心任务，即将原始数据的“含义”表示为一组数字。这就是为什么语义搜索效果如此出色的原因。

import WhatNext from '/_includes/what-next.mdx'

<WhatNext />