---
authors:
- connor
date: 2022-10-04
description: Learn about the various Sentence Transformers from Hugging Face!
image: ./img/hero.png
slug: how-to-choose-a-sentence-transformer-from-hugging-face
tags:
- integrations
title: How to choose a Sentence Transformer from Hugging Face
---

![如何从Hugging Face选择一个Sentence Transformer](./img/hero.png)

<!-- 截断 -->

[Weaviate](/developers/weaviate/)最近推出了一个新的模块，允许用户轻松地将[Hugging Face](/blog/hugging-face-inference-api-in-weaviate)的模型集成到他们的数据和查询中进行向量化。在撰写本文时，有[超过700个模型](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads)可以轻松地插入到Weaviate中。

你可能会问：为什么有这么多模型，它们有什么区别？<br/>
更重要的是：如何选择一个用于语义搜索的句子转换器？

有太多的模型无法在一个流程图中总结。因此，我们将描述区分这些模型的因素，并为您提供工具来选择适合您使用场景的完美模型。

<img
    src={require('./img/huggingface-sentence-transformers.png').default}
    alt="Weaviate Sentence Transformers"
    style={{ maxWidth: "70%" }}
/>

## 深度学习模型的差异

不久之前，深度学习模型通常是根据架构决策来区分的。例如，ResNet和DenseNet之间的区别在于它们在层之间实现跳跃连接的频率。快进到今天，深度学习社区对注意力层和Transformer网络架构深感兴趣。Transformer主要在编码器、解码器和编码器-解码器设计方面有所不同。它们还在层的数量和隐藏维度大小等细节上有所变化。然而，由于Hugging Face transformers库的美妙之处以及该特定模型架构的成功，对这些细节的考虑大多已成为过去。这些细节通常可以归纳为“参数数量”指标，其中大多数句子转换器包含约2200万个参数。

这些天，神经网络之间的差异更多地基于它们训练的**数据**，以及一些关于它们如何训练的细微差别，我们将在未来的文章中详细介绍。本文将重点介绍**4个关键维度**：

1. [领域](#domain)
2. [任务](#task)
3. [规模](#scale)
4. [形式](#modality)

## 领域
**领域**在很大程度上描述了数据集所涉及的高层概念。这是机器学习研究中常用的术语，用于描述大量对象的差异。例如，在计算机视觉中，一个**领域**可能是画笔插图，另一个**领域**可能是照片般逼真的图像。无论是画笔插图还是照片般逼真的图像都可以被标记为相同的**任务**，即将图像分类为猫或狗。如果最终使用场景是照片般逼真的图像，那么用于分类画笔插图中的猫的模型将不如用于分类照片般逼真图像中的猫的模型表现得好！

在自然语言处理（NLP）中，领域差异非常常见，例如法律合同、财务报表、生物医学科学论文、维基百科或Reddit对话等之间的差异。

### 颜色编码的详细信息
对于每个模型，Hugging Face显示了一系列重要的**颜色编码**详细信息，例如：
* 蓝色 - 它所训练的**数据集**
* 绿色 - 数据集的**语言**
* 白色或紫色 - 关于模型的**附加细节**

因此，如果我们看两个深度学习模型，我们可以看到[dangvantuan/sentence-camembert-large](https://huggingface.co/dangvantuan/sentence-camembert-large)是在**stsb_multi_mt**上进行训练的，这是一个**法语**数据集。

![Camembert Hugging Face模型卡片](./img/camembert-model-card-min.png)

而[sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)则是在**多个英语数据集**上进行训练的。

![all-MiniLM Hugging Face Model Card](./img/minilm-model-card-min.png)

直截了当地说，使`dangvantuan/sentence-camembert-large`在法语句子嵌入方面比`sentence-transformers/all-MiniLM-L6-v2`更好的原因是...它是在**法语**句子上训练的！类似这样的例子还有很多，通常在特定领域进行了明确训练的模型，如**生物医学文本**、**法律文件**或**西班牙语**，在测试该领域时通常比未经明确训练的模型表现更好。

请注意，这些标签是Hugging Face的**模型卡片**的一部分，这是一个令人印象深刻的努力，旨在持续改进机器学习模型的组织。在撰写本文时，模型卡片仍然依赖于**手动标记**。可能开发者上传模型时没有填写这些细节。如果您是Hugging Face的新用户，请考虑通过[添加模型卡片](https://huggingface.co/docs/hub/model-cards)来对上传的模型进行注释 - 在`README.md`中添加YAML部分，就像这样：

<img
    src={require('./img/how-to-populate-model-card-min.png').default}
    alt="如何填充Hugging Face模型卡片"
    style={{ maxWidth: "70%" }}
/>

### 私有模型
Weaviate与Hugging Face的集成之一的优点是**任何人**都可以将自己的模型上传到Hugging Face，并在Weaviate的向量数据库中使用它们。例如，我正在研究COVID-19文献，所以我已经对CORD-19标题和摘要进行了微调，并将其上传到[CShorten/CORD-19-Title-Abstracts](https://huggingface.co/CShorten/CORD-19-Title-Abstracts-1-more-epoch)。

### 领域回顾
简单回顾一下，**领域**主要描述了数据集的高层概念，即数据集关注的是**什么**。除了领域之外，还有许多用于生成向量嵌入的**任务**。与语言模型不同，大多数模型在训练任务中使用的是"预测被屏蔽的标记"。嵌入模型的训练方式更加广泛。例如，**重复问题检测**可能比使用**问答**训练的模型表现更好。

在选择模型时，通常最好选择与您的用例在相同领域内训练的模型。另一个重要的考虑因素是用于标记数据的任务，以及这种标记是否与所需任务相一致。如果我们有两个训练于**法律合同**的模型，并且我们想要开发一个**问答系统**...理想情况下，最好选择一个不仅在**法律合同**上进行了训练，而且还进行了**问答**训练的HuggingFace模型。

## 任务
**任务**是模型训练的函数，例如问答或图像分类。Sentence Transformers和其他嵌入模型（如CLIP）解决了预测哪个数据点与查询**相似**，哪个或哪些数据点与查询**不相似**的**任务**。这与GPT和BERT风格的模型不同，后者完成了预测遮蔽标记的**任务**。

在深度学习应用中，监督学习数据集在句子转换模型中起着重要作用，尤其是对于许多问题回答模型而言。与图像分类甚至重复问题检测相比，标记为问题回答的数据集非常不同。

### 任务基准测试
两个优秀的基准测试用于收集有监督学习任务，以评估句子转换模型，分别是知识密集型语言任务（[KILT](https://ai.facebook.com/tools/kilt/)）和信息检索基准测试（[BEIR](https://arxiv.org/abs/2104.08663)）。

**KILT** 对于所有任务都使用相同的领域，即维基百科。每个任务都有不同的标签：槽填充、问题回答、对话、事实检查或实体链接。

![KILT 基准测试](./img/KILT-min-correct.png)

**BEIR**测试了许多不同的任务：事实检查、引文预测、重复问题检索、论证检索等。BEIR还测试了包括维基百科、科学论文、Quora、Stack Exchange、互联网抓取、新闻、财务文件和推特在内的多个领域。

![BEIR基准测试](./img/BEIR-min.png)

### 基于真实人类查询的数据集
**[MS MARCO](https://arxiv.org/abs/1611.09268)** 是另一个具有影响力的数据集，其中包含了微软必应搜索引擎上真实的人类查询与用户点击的段落，希望能够回答他们的查询。另一个例子是 **[Quora Question Pairs](https://arxiv.org/abs/1907.01041)**，其中包含了人工标记的两个问题是否在询问相同的事情。Sentence Transformer的文档介绍了5个主要的训练数据类别：语义文本相似度（STS）、自然语言推理（NLI）、释义数据、Quora重复问题（QQP）和MS MARCO。

这些数据集可以简化为两个类别：(1)**精确语义匹配**和(2)**关系推理**。我认为采用这些类别可以简化用于嵌入优化的文本数据集的组织。

**精确语义匹配**旨在包含STS和Quora问题对等数据集，以及其他各种单语和多语言的释义数据集。

**关系推理** 旨在涵盖自然语言推理、事实验证、问答数据集、点击日志（如MS MARCO）以及从网页抓取中提取的大量弱监督数据集，例如Reddit对话和Stack Exchange讨论。

我认为**精确语义匹配**和**关系推理**之间的核心区别在于任务是否需要一定程度的中间关系处理。例如，在自然语言推理任务中，仅仅说"博文喜欢冰淇淋"等同于"我看到博文吃冰淇淋"是不够的。模型必须学习一个中间的蕴含关系函数。相比于将英语对齐到法语翻译或检测问题的释义等任务，自然语言推理、事实验证、问题回答等任务更需要学习这些中间函数。

### 问答与事实验证的区别
这两个高级类别是对这些数据集进行分类的好的起点。然而，在每个类别内部仍然存在明显的差异。例如，评估句子转换器的质量时，什么使得**问答**与**事实验证**不同？简单来说，问题与事实有着独特的风格。因此，虽然检索任务类似于返回最合适的答案或证据，但问题与陈述事实之间的细微差别会对性能产生影响。我们建议读者在选择适合自己用例的句子转换器时注意这些微妙的差异。

### 任务构建
在任务构建中，另一个重要的概念是讨论监督学习和自监督学习，这也很好地引出了下一个主题——**规模**。监督学习需要人类对数据进行标注，而自监督学习则不需要。自监督学习使我们能够利用互联网规模的数据。然而，与自回归型的GPT算法不同，嵌入学习在自监督数据中还没有收敛到单一的任务上。

最常见的自监督任务启发式方法是使用相邻的段落作为正对。这种方法非常有效，因为大多数相邻的段落往往在语义上相关。如下所示的1B训练对示例，互联网上也存在许多类似的情况，例如Reddit帖子-评论对或Stack Exchange问题-答案对。

### 任务回顾
通常，在选择适合您用例的句子转换器时，您希望在众多预训练模型中找到**领域**和**任务**之间的最佳匹配。另一个需要了解的关键因素是训练中使用的数据集的**规模**。深度学习模型通常受益于规模较大的数据集，但这可能会导致标注噪声增加，尤其是当模型大小没有与数据集相匹配时。

## 规模
在深度学习中，**规模**通常指的是更大的模型，但在这里我们使用规模来描述数据集的大小。当然，这些因素是相互纠缠在一起的。在撰写本文时，通常需要增加模型的规模以充分利用更大的数据集。因此，如果受到预算限制，高质量的数据集就变得更加重要。

在为本文研究嵌入数据集的**规模**时，我发现了一个非常有趣的事情，那就是Hugging Face社区举办的一个活动，目标是使用10亿个训练对训练出最佳的句子嵌入模型，该活动由Nils Reimers领导。在这10亿个对中，以下一些子数据集引起了我的注意：来自2015年至2018年的Reddit评论，有**约7.3亿个对**！S2ORC引文对约**1.16亿个**，WikiAnswers重复问题对约**7700万个**，Stack Exchange对约**2500万个**，还有一些带有标签的数据集，如Quora问题三元组和Natural Questions，每个数据集约有**10万对**。我强烈建议查看完整的列表，可以在[这里](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2)找到。

### 弱监督学习或自监督学习技术的吸引力
1B训练对挑战数据集的清单展示了弱监督或自监督学习技术的吸引力。相比之下，自然问题是用于问答任务的较大的有监督数据集之一。自然问题（NQ）需要人类根据维基百科的上下文输入推导问题。由于这个过程较慢，数据集包含了100,231个问题-上下文-答案元组... 这相当令人印象深刻，但比起从Reddit中提取的约7.3亿个对而言，规模要小得多，而且不需要人工干预。

### 规模 vs 模型适配性
大规模和昂贵的模型可以通过像7.3亿Reddit对话这样的数据集实现出色的能力。然而，如果我们受到Sentence Transformers平均约2200万参数的限制，并且在一个通用问答应用程序中使用Weaviate，那么自然问题中的10万个示例将比Reddit对话对于该特定用例产生更好的模型。

### 规模回顾
在考虑**规模**时，我们通常希望有尽可能多的数据。但是，如果我们从互联网对话线程等嘈杂的来源获得数据，将其扩展的成本可能不值得。

## 多样性
模态性是深度学习中最令人兴奋的新兴趋势之一，如果在本次讨论中不提及它将是遗憾的。像DALL-E这样的模型可以通过文本提示生成令人难以置信的图像，或者像CLIP这样的模型可以通过自然语言界面搜索大规模的图像集。这些模型引起了我们对深度学习未来的想象。读者可能会很高兴地知道，[Sentence Transformer库支持CLIP模型](https://huggingface.co/sentence-transformers/clip-ViT-B-32)。

深度学习研究主要集中在文本、图像和图像-文本的组合上，但是这个领域正在迅速发展，取得了在视频、音频、蛋白质、化学等方面的成功。我特别着迷于文本-代码多模态空间以及将这种模态组合插入到Weaviate的向量数据库特性中的机会。

## 结论
简而言之，HuggingFace的句子转换器检查点主要在于它们所训练的**数据**不同。选择与您的用例最匹配的模型是通过识别最相似的领域和任务来确定的，同时欢迎更多的数据集规模。大多数使用句子转换器的Weaviate应用主要关注文本、图像或图像-文本空间的**模态**，但这些类别的数量在不久的将来可能会激增。


import WhatNext from '/_includes/what-next.mdx'

<WhatNext />