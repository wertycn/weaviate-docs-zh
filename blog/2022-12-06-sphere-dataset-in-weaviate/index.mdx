---
authors:
- zain
date: 2022-12-6
description: Learn how to import and query the Sphere dataset in Weaviate!
image: ./img/hero.png
slug: sphere-dataset-in-weaviate
tags:
- how-to
title: The Sphere Dataset in Weaviate
---

![Weaviate中的Sphere数据集](./img/hero.png)

<!-- 截断 -->

## 什么是Sphere？
[Sphere](https://github.com/facebookresearch/sphere) 是由 Meta 最近发布的一个开源数据集。它包含了1.34亿个文档（分成了9.06亿个100个单词的片段）。这是一个巨大的知识库，可以帮助解决知识密集型的自然语言任务，如问答、事实核查等。您可以通过[这里](https://ai.facebook.com/blog/introducing-sphere-meta-ais-web-scale-corpus-for-better-knowledge-intensive-nlp/)了解更多信息。

简而言之，Sphere旨在成为一个"通用、非筛选和非结构化的知识来源"。这意味着下次当你有一个问题，比如："麦当劳这个连锁餐厅是由那个养农场的老麦当劳创建的吗？" Sphere将会有相关的知识来回答你的问题，并指向一个相关的文章。这个庞大的数据集的潜力令人惊叹，人们可以无限地设想各种应用 - 从在社交媒体平台上打击假新闻，到帮助找到你下一个梦想的度假胜地。

另外，Sphere非常适合大规模的混合向量搜索，因为它是少数几个提供向量及其对应文本字段的大规模数据集之一。您可以在[这里](https://huggingface.co/facebook/dpr-question_encoder-single-nq-base)了解用于生成这些向量的模型。出于这些原因，我们希望尽可能让这个资源对社区更加可访问。

## 使用Sphere的挑战

这个数据集的唯一限制是对于普通开发者来说，获取和使用起来非常困难。在这方面，数据集的庞大规模成为了一把双刃剑。对于除了大型工业和学术实验室之外的任何人来说，使用Sphere的当前开源格式都具有挑战性 - 即使对于它们来说，用户体验还有很大的提升空间。

*不相信我吗？*

请尝试按照 [readme](https://github.com/facebookresearch/sphere) 的指示，在您的机器上设置并使用 Sphere。您将首先面临的限制是，最小的开源稀疏 Sphere 索引文件在压缩格式下达到了惊人的 833GB。一旦您跨过了这个障碍，为了开始使用 Sphere 数据集来进行混合搜索测试和基准测试，还需要付出另一番艰巨的努力。

## Weaviate 中的 Sphere 数据集
为了使这一强大资源能够为每个人所用，我们很高兴地宣布，Sphere现在不仅可以在Weaviate中使用，还可以作为JSON或Parquet文件提供。可以使用Python和Spark轻松导入数据集！您可以导入Sphere的大型矢量化块（或者整个数据集，如果您愿意！），并只需几行代码即可开始搜索！

太阳的力量尽在您的手中！*随之而来的是邪恶的狂笑声*

import sphereVideo from './img/joke2.mp4';

<video width="100%" autoplay loop controls>
  <source src={sphereVideo} type="video/mp4" />
Your browser does not support the video tag.
</video>


明白了吗？...这是一个球体🥁咚咚咚🥁我会自己走开的...

有两种方法可以将Sphere数据集导入到Weaviate中。您可以使用Python客户端（少于75行代码）或Weaviate Spark连接器。

### 使用Python导入Sphere数据集
安装非常简单，您只需要[Weaviate客户端](/developers/weaviate/client-libraries)。我们提供了一个使用Python客户端和dpr-question_encoder-single-nq-base模型的示例（即用于在Sphere中对对象进行向量化的模块）。

我们准备了从100K数据点到完整的Sphere数据集（包含899亿行）的文件。您可以从这里下载它们：
* [100k行](https://storage.googleapis.com/sphere-demo/sphere.100k.jsonl.tar.gz)
* [1M行](https://storage.googleapis.com/sphere-demo/sphere.1M.jsonl.tar.gz)
* [10M行](https://storage.googleapis.com/sphere-demo/sphere.10M.jsonl.tar.gz)
* [100M行](https://storage.googleapis.com/sphere-demo/sphere.100M.jsonl.tar.gz)
* [899M行](https://storage.googleapis.com/sphere-demo/sphere.899M.jsonl.tar.gz)

一旦您下载和解压缩了数据集文件，下一步是使用Python将数据集导入Weaviate。

```python
import sys
import os
import time
import json
import weaviate

# Variables
WEAVIATE_URL    = 'https://loadtest.weaviate.network/'
BATCH_SIZE      = 100
SPHERE_DATASET  = 'sphere.100k.jsonl' # update to match your filename

client = weaviate.Client(
    url=WEAVIATE_URL,
    timeout_config=600
)

client.batch.configure(
    batch_size=BATCH_SIZE,
    dynamic=True,
    num_workers=os.cpu_count()
)

# Set DPR model used for the Page class
client.schema.create_class({
    "class": "Page",
    "vectorizer": "text2vec-huggingface",
    "moduleConfig": {
        "passageModel": "sentence-transformers/facebook-dpr-ctx_encoder-single-nq-base",
        "queryModel": "sentence-transformers/facebook-dpr-question_encoder-single-nq-base",
        "options": {
            "waitForModel": True,
            "useGPU": True,
            "useCache": True
        }
    },
    "properties": []
})

# Import the data, Weaviate will use the auto-schema function to
# create the other properties and other default settings.
start = time.time()
c = 0
with open(SPHERE_DATASET) as jsonl_file:
    with client.batch as batch:
        for jsonl in jsonl_file:
            json_parsed = json.loads(jsonl)
            batch.add_data_object({
                    'url':  json_parsed['url'],
                    'title': json_parsed['title'],
                    'raw': json_parsed['raw'],
                    'sha': json_parsed['sha']
                },
                'Page',
                json_parsed['id'],
                vector=json_parsed['vector']
            )
            c += 1
            if (c % (BATCH_SIZE * 10)) == 0:
                print('Imported', c)

end = time.time()
print('Done in', end - start)
```
*确保将SPHERE_DATASET属性更新为正确匹配您的`.jsonl`文件名。*

### 使用Spark导入Sphere
如果您想在Sphere上开始训练用于知识密集型任务的大型语言模型，那么您可能希望利用大数据框架。这就是Apache Spark的用武之地！

要使用Spark处理Sphere，您可以使用[PySpark](https://spark.apache.org/docs/latest/api/python/)和[Weaviate的Python客户端](/developers/weaviate/client-libraries/python)。设置过程稍微复杂一些，不仅仅是简单地使用Python导入数据集；然而，一旦设置完成，它的速度非常快！⚡

您可以在这个[tutorial](/developers/weaviate/tutorials/spark-connector)中看到详细的一步一步的说明。这个教程演示了如何将Sphere导入到Spark dataframe中，然后将其导入到Weaviate，并进行查询。一旦您将Sphere导入到您的Spark实例中，您就可以利用Spark的功能来开始训练强大的模型。在这个特定的示例中，我们使用了[Weaviate Spark连接器](https://github.com/weaviate/weaviate-spark-connector)，使从Spark加载数据到Weaviate变得非常容易。

我们还准备了两个Parquet文件，一个包含1百万个数据点，另一个包含整个Sphere数据集，该数据集包含8.99亿行。您可以按照以下方式将它们下载到数据框中：

```
df = spark.read.parquet("gs://sphere-demo/parquet/sphere.1M.parquet")
df = spark.read.parquet("gs://sphere-demo/parquet/sphere.899M.parquet")
```
## 使用 Weaviate 在 Sphere 中进行搜索
现在，指令已经完成了，让我们来玩一下，展示一下 Weaviate 在 Sphere 上的综合能力！我们将整个 Sphere 数据集导入到了 Weaviate 中 - 是的，所有约 8.99 亿个对象，下面是证据！

![导入证明](./img/image4.png)

一旦 Sphere 数据集在 Weaviate 中，我们就可以与 Weaviate 提供的所有功能一起使用它。

由于Sphere的主要用途是进行大规模混合搜索，下面是一个示例，我们利用向量搜索来找出Sphere认为意大利最好吃的食物，同时使用传统的词匹配来确保返回的对象来自可靠的来源，本例中是纽约时报。

![查询](./img/image1.png)

这就是全部内容！现在，有了Sphere数据集，可以轻松导入到Weaviate中，任何人都可以开始利用这个强大的工具，结合我们在Weaviate中已经提供的许多令人惊喜的功能来构建。祝您编码愉快！

import StayConnected from '/_includes/stay-connected.mdx'

<StayConnected />