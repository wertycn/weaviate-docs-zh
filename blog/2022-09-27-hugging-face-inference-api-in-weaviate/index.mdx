---
authors:
- sebastian
date: 2022-09-27
description: Running ML Model Inference in production is hard. You can use Weaviate
  – a vector database – with Hugging Face Inference module to delegate the heavy lifting.
image: ./img/hero.png
slug: hugging-face-inference-api-in-weaviate
tags:
- integrations
title: Support for Hugging Face Inference API in Weaviate
---

![在 Weaviate 中支持 Hugging Face 推理 API](./img/hero.png)

<!-- truncate -->

向量数据库使用机器学习模型为您的数据提供令人难以置信的功能。我们可以看到各种用例，从**摘要生成器**（可以将任何文本摘要成一句简短的句子），到**自动标记器**（可以对数据标记进行分类），再到**转换器**和**向量化器**（可以将任何数据 - 文本、图像、音频等 - 转换为向量，并用于基于上下文的查询）。

所有这些用例都需要进行`机器学习模型推理`- 通过一个ML模型运行数据并计算输出的过程（例如，将一个段落总结为一个简短的句子）- 这是一个计算密集型的过程。

### 问题中的难题
在生产环境中运行模型推理是困难的。
* 需要昂贵的专用硬件。
* 在初始数据导入过程中需要更多的计算能力。
* 一旦大部分繁重的工作完成，硬件往往被低效利用。
* 与其他团队共享和优先处理资源很困难。

好消息是，有一些公司，比如Hugging Face、OpenAI和Cohere，提供模型推理作为一项服务。

> "在生产环境中运行模型推理是困难的，
让他们为您完成吧。"

## Weaviate对Hugging Face推理API的支持
从Weaviate `v1.15`开始，Weaviate包含了一个Hugging Face模块，该模块可以直接从矢量数据库中提供对Hugging Face Inference的支持。

Hugging Face模块允许您使用[Hugging Face Inference服务](https://huggingface.co/inference-api#pricing)和句子相似性模型，直接从Weaviate对数据进行向量化和查询。无需自行运行Inference API。

> 您可以选择`text2vec-huggingface`（Hugging Face）和`text2vec-openai`（OpenAI）模块来委托您的模型推理任务。<br/>
> 这两个模块在[Weaviate云服务](/pricing)中都是默认启用的。

## 概述
![概述](./img/hugging-face-module-overview.png)

Hugging Face模块非常令人难以置信，有很多原因。

### 公共模型
您可以访问超过1600个预训练的[句子相似性模型](https://huggingface.co/models?pipeline_tag=sentence-similarity)。如果已经有一个适用于您的用例的模型，就不需要训练自己的模型。

如果您在选择合适的模型方面遇到困难，可以查看我们的博客文章[如何从Hugging Face选择句子转换器](/blog/how-to-choose-a-sentence-transformer-from-hugging-face)。

### 私有模型
如果您有自己的模型，专门为您的数据进行训练，那么您可以将它们上传到Hugging Face（作为私有模块），并在Weaviate中使用它们。

<!-- TODO: 更新文章链接一旦准备好 -->
*我们正在编写一篇文章，将指导您如何创建自己的模型并将其上传到Hugging Face。*

### 全自动化和优化
Weaviate会为您管理整个过程。从编写代码的角度来看，一旦您有了模式配置，几乎可以忘记Hugging Face的存在。

例如，当您将数据导入到Weaviate时，Weaviate会自动提取相关的文本字段，将它们发送给Hugging Face进行向量化，并将数据与新的向量一起存储在数据库中。

### 最简单易用的准备就绪
每个使用[Weaviate Cloud Services](/pricing)创建的新Weaviate实例都默认启用了Hugging Face模块。您无需更新任何配置文件或其他内容，它已经准备就绪。

另外，要在Weaviate开源版（`v1.15`或更新版本）中使用Hugging Face模块，您只需将`text2vec-huggingface`设置为默认的向量化器。如下所示：

```yaml
DEFAULT_VECTORIZER_MODULE: text2vec-huggingface
ENABLE_MODULES: text2vec-huggingface
```

## 如何入门

:::note
This article is not meant as a hands-on tutorial.
For more detailed instructions please check the [documentation](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface).
:::

使用Hugging Face模型与Weaviate的整体过程非常简单。

![使用Hugging Face模型的步骤](./img/how-to-get-started-recipe.png)
如果这是一堂烹饪课，您正在按照食谱操作。

您需要准备以下材料：
* 原始数据
* Hugging Face API令牌 - 您可以从[官方网站](https://huggingface.co/settings/tokens)上申请
* 已启用“text2vec-huggingface”的可工作的Weaviate实例

然后您将按照以下步骤进行操作。

### 第1步 - 初始准备 - 创建模式和选择hf模型
一旦您启动并运行了Weaviate实例。
定义您的模式（标准的操作 - 选择类名、属性和数据类型）。作为模式定义的一部分，您还需要提供每个模式类别所要使用的Hugging Face模型。

通过向模式定义中添加一个`moduleConfig`属性和`model`名称来完成此操作，如下所示：
```javascript
{
    "class": "Notes",
    "moduleConfig": {
        "text2vec-huggingface": {
            "model": "sentence-transformers/all-MiniLM-L6-v2",  # model name
            ...
        }
    },
    "vectorizer": "text2vec-huggingface",  # vectorizer for hugging face
   ...
}
```

*如果你想知道的话，是的，你可以为每个类别使用不同的模型。*

### 第二步 - 煮一段时间 - 导入数据
开始将数据导入到Weaviate中。

为此，您需要您的Hugging Face API令牌，该令牌用于授权所有与🤗的调用。

将您的令牌添加到Weaviate客户端配置中。例如，在Python中，您可以像这样做：

```javascript
client = weaviate.Client(
    url='http://localhost:8080',
    additional_headers={
        'X-HuggingFace-Api-Key': 'YOUR-HUGGINGFACE-API-KEY'
    }
)
```
然后按照以往的方式导入数据。Weaviate将处理与Hugging Face的所有通信。

### 第3步 - 服务部分 - 查询数据
一旦导入了部分或全部数据，您就可以开始运行查询。
（是的，在导入过程中，您甚至可以开始查询您的数据库）。

运行查询也需要相同的令牌。
但是您可以重用同一个客户端，所以您可以开始了。

然后，您只需按照通常的方式运行查询：
```javascript
nearText = {
    "concepts": ["How to use Hugging Face modules with Weaviate?"],
    "distance": 0.6,
}

result = (
    client.query
    .get("Notes", [
        "name",
        "comment",
        "_additional {certainty distance} "])
    .with_near_text(nearText)
    .do()
)
```

## 摘要
> 现在您可以在Weaviate中使用[Hugging Face](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface)或[OpenAI](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai)模块来委托模型推理。

只需选择模型，提供您的API密钥，即可开始处理您的数据。

Weaviate为您优化了与推理API的通信过程，这样您就可以专注于应用程序的挑战和需求。无需自己运行推理API。

## 接下来做什么
查看[text2vec-huggingface](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface)文档，了解更多关于新模块的信息。

如果您是Weaviate的新手，请查看[入门指南](/developers/weaviate/quickstart)。

如果您觉得这篇文章有趣或有用，请告诉我们。我们非常乐意接收建设性的反馈意见。😀

我们一直在撰写新的文章，并且正在寻找新的主题。如果有任何您希望我们写的内容，请告诉我们。🤗

import ShareFeedback from '/_includes/share-feedback.md';

<ShareFeedback />